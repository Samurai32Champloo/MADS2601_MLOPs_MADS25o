\section{MLOps als Rahmenwerk für den zuverlässigen Betrieb von ML-Modellen}
Ein ganzheitlicher Blick auf MLOps ist zentral, um die vielfältigen Zusammenhänge zwischen organisatorischen Abläufen,
technischen Komponenten und grundlegenden Prinzipien moderner ML-Systeme zu verstehen. 
MLOps wird dabei als Rahmen betrachtet, der die bestehenden Herausforderungen im Umgang mit datengetriebenen Modellen strukturiert adressiert und zugleich Orientierung für robuste, skalierbare und nachvollziehbare Prozesse bietet. 
Die folgende Ausarbeitung zielt darauf ab, die wesentlichen Prozessschritte, Rollen und architektonischen Bausteine in einer abstrahierten, allgemein übertragbaren Form zusammenzuführen. 
Zugleich bleibt MLOps ein flexibel adaptierbares Konzept, dessen konkrete Umsetzung stets von den organisatorischen, technischen und projektbezogenen Rahmenbedingungen abhängt.


\subsection{Das ganzheitliche MLOps-Workflow-Modell}
Der Prozess der Operationalisierung von maschinellen Lernverfahren erfordert eine klar strukturierte Abfolge von Aufgaben,
die sowohl die Automatisierung als auch die Integration von Modellen in produktive Systeme adressieren. 
Ziel von MLOps ist es, manuelle Arbeitsschritte innerhalb von ML-Prozessen zu reduzieren und die Überführung von Proofs of Concept in die Produktion zu erleichtern \cite{kreuzberger2023}.

In der Literatur werden vergleichbare Prozessstrukturen beschrieben, die jedoch in Bezeichnung, Schwerpunktsetzung und Integration der einzelnen Schritte variieren. 
Grundlegende Phasen wie Datenvorbereitung, Feature Engineering, Modellentwicklung, Deployment und Monitoring werden in allen Arbeiten behandelt, 
unterscheiden sich jedoch in der Abfolge, Unterteilung in Aufgabenpakete,
der Verzahnung mit DevOps-Praktiken sowie der Einbindung kontinuierlicher Feedback- und Qualitätskontrollmechanismen \cite{kreuzberger2023,wozniak2025,wazir2023,berberi2025}. 

So betonen Wozniak et al. die klare Trennung zwischen Daten- und Modellvorbereitung, gefolgt von Deployment und Monitoring,
während Wazir et al. eine initiale Anforderungsanalyse vorsehen und die Datensammlung, Datenaufbereitung, Feature Engineering sowie das Training und die Evaluation der Modelle als eine zusammenhängende Phase der Modellentwicklung betrachten.
Aus der Analyse der unterschiedlichen Prozessmodelle lässt sich eine abstrahierte Sicht auf MLOps gewinnen, 
in der die zentralen Aktivitäten in vier übergeordnete Funktionsbereiche eingeordnet werden können, nämlich die \textit{MLOps Produktinitialisierung}, den Bereich des \textit{Data Engineering} sowie die Modellentwicklung, 
die in der Architektur zugleich als \textit{Experimentierzone} bezeichnet wird, und die \textit{Workflow Automatisierung} umfassen

Diese Einteilung bietet einen strukturierten Rahmen, um die unterschiedlichen Prozessmodelle aus der Literatur einzuordnen und die zentralen Zusammenhänge zwischen Aufgaben, 
Rollen und architektonischen Bausteinen sichtbar zu machen.

Aufbauend auf diesen Überlegungen zeigt die generalisierte MLOps End-to-End Architektur von Kreuzberger et al. den gesamten Ablauf vom Start eines MLOps-Produkts bis zur Modell Bereitstellung \cite{kreuzberger2023}.

\subsection{MLOPs Produkt Initialisierung}
Wozniak et al. haben in einer Literaturanalyse verschiedener MLOps Prozessmodelle identifiziert und analysiert,
welche Aufgabenpakete sich in der Literatur häufig wiederfinden. Dabei finden sie zunächst eine einleitende Stufe,
die sich mit der Analyse des Geschäftsproblems befasst. Obwohl die untersuchten Veröffentlichungen diesen Schritt nicht immer vertiefen,
zeigt die Übersicht dennoch eindeutig, dass eine betriebliche Problem und Zielanalyse in nahezu allen MLOps Ansätzen berücksichtigt wird \cite{wozniak2025}.

Alle der betrachteten Veröffentlichungen beziehen sich in den jeweiligen MLOPs Ansätzen als Grundlage auf den Cross Industry Standard Process for Data Mining (CRISP-DM) oder den Team Data Science Process (TDSP). 
Auch CRISP-DM beginnt mit einer initialen Phase, in der Ziele bestimmt, Rahmenbedingungen geklärt und Projektziele definiert werden \cite{karamitsos2020}. 
Obwohl CRISP-DM nicht speziell für MLOps entwickelt wurde, macht es bereits an zentraler Stelle die Notwendigkeit einer strukturierten Initialisierung sichtbar. 
TDSP führt diese Logik fort und erweitert sie um klar definierte Stakeholder und messbare Erfolgsindikatoren \cite{karamitsos2020}. 
Beide Vorgehensmodelle zeigen, dass datengestützte Projekte traditionell mit einer präzisen Klärung von Zielen und Anforderungen beginnen.

Testi et al. entwickeln auf Basis einer Literaturrecherche einen MLOps Ansatz, der die Phase des Geschäftsverständnisses ebenfalls an den Anfang stellt \cite{testi2022}. 
Diese Phase umfasst die Sammlung und Dokumentation von Anforderungen, die Bestimmung messbarer Erfolgsindikatoren sowie die Ermittlung relevanter Daten.
Dabei sind die festgelegte Ziele und geeignete Metriken später die Grundlage für das Monitoring produktiver ML Systeme. 

Eine technisch orientierte Ergänzung liefern Bachinger et al. \cite{bachinger2024}. Sie zeigen, dass ein MLOps Prozess nicht nur durch ein geschäftliches Problem ausgelöst werden kann, 
sondern auch durch technische Signale innerhalb einer automatisierten Pipeline. 
Die Identifikation neuer Daten, Strukturänderungen in vorhandenen Daten oder externe Aktualisierungen können den Workflow automatisch anstoßen. 
Dadurch erhält die Projektinitialisierung eine dynamische Komponente, die eng mit Prinzipien kontinuierlicher Integration und Automatisierung verbunden ist.

Die Zusammenführung aller genannten Quellen zeigt ein konsistentes Muster. 
Jede Form eines datengetriebenen Projekts beginnt mit der Klärung des zugrunde liegenden Problems, 
der Definition der Ziele und einer Analyse der verfügbaren Daten.

Die generalisierte Architektur nach Kreuzberger et al. erweitert dieses Muster um die Prinzipien der Automatisierung, Skalierbarkeit und Betriebssicherheit. 
Diese Projektinitialisierung wird in der MLOps End-to-End Architektur dabei als MLOPs Produkt Initialisierung beschrieben. 
Zunächst wird ein Geschäftsproblem identifiziert und analysiert. Anschließend wird eine Lösungsarchitektur entworfen, auf deren Grundlage die geeignete algorithmische Methodik festgelegt wird.
Danach klären die Stakeholder gemeinsam den Datenbedarf, prüfen verfügbare Quellen, 
bewerten deren Qualität und prüfen, ob die Daten die fachlich benötigte Aussagekraft für die Beantwortung des Geschäftsproblems besitzen.
Diese Aufgaben erzeugen die fachliche und technische Basis für die spätere Modellierung und bereiten den stabilen Produktivbetrieb vor\cite{kreuzberger2023}.

\subsection{Data Engineering}

Auch die Phase der Datenvorbereitung ist ein zentraler Bestandteil aller betrachteten datengetriebenen Prozessmodelle. 
In der Literatur wird sie jedoch unterschiedlich verortet. Einige Veröffentlichungen behandeln die Datenvorbereitung als eigenständige Prozessphase \cite{kreuzberger2023, wozniak2025, testi2022, john2021}, 
während andere sie der Modellentwicklungsphase zuordnen \cite{wazir2023}. 
Wiederum andere Modelle differenzieren stärker und trennen zwischen einer Phase des Datenverständnisses und einer Phase der Datenvorbereitung, wie es beim CRISP-DM Ansatz der Fall ist \cite{bachinger2024, karamitsos2020}.

In der End-to-End Architektur nach Kreuzberger et al. ist der Bereich des Data Engineerings in eine Anforderungspipeline und eine Feature Engineering Pipeline unterteilt \cite{kreuzberger2023}. 
In der Anforderungspipeline wird die Grundlage für die spätere Modellierung geschaffen, indem Rohdaten systematisch transformiert, bereinigt und in eine belastbare Struktur überführt werden. 
Zunächst werden die grundlegenden Anforderungen für die Feature Engineering Pipeline definiert. 
Dazu gehören Regeln für Transformationen wie Normalisierung oder Aggregationen sowie Vorgaben zur Bereinigung, um die Daten in ein verlässlich nutzbares Format zu überführen. 
Diese Regeln werden fortlaufend verfeinert, sobald Rückmeldungen aus den nachgelagerten Bereichen der Modellentwicklung oder aus dem Monitoring der Produktivmodelle vorliegen \cite{kreuzberger2023}.

In der Feature Engineering Pipeline werden diese Anforderungen technisch umgesetzt. 
Die zuvor festgelegten Regeln ermöglichen eine automatisierte Datenverarbeitung. 
Rohdaten können damit immer wieder in dasselbe Ausgangsformat überführt werden, was die Reproduzierbarkeit der Datengrundlage sicherstellt. 
Bei Veränderungen der Rohdaten ist gewährleistet, dass die nachfolgenden Bereiche auf einer stabilen und konsistenten Datenstruktur aufbauen können. 
Die Rohdaten können dabei aus dynamischen Streamingquellen wie Echtzeitdaten stammen oder als statische Batchdaten vorliegen, die einmalig eingespielt oder in regelmäßigen Abständen aktualisiert werden. 
Im Rahmen der Pipeline werden sie extrahiert, transformiert und bereinigt, wobei die zuvor definierten Regeln zur Anwendung kommen und fortlaufend auf Basis Rückmeldungen weiter verfeinert werden \cite{kreuzberger2023}.

Darauf folgt die Berechnung weiterführender Features auf Grundlage der transformierten und bereinigten Daten sowie der festgelegten Ableitungsregeln, 
die kontinuierlich auf Basis neuer Erkenntnisse aus nachgelagerten Bereichen weiterentwickelt werden. 
Abschließend werden die erzeugten Features über einen Ingestionprozess in ein zentrales Feature Store System geladen. 
So entsteht eine robuste und reproduzierbare Datenbasis, die konsistent für Trainingsprozesse und produktive Anwendungen bereitgestellt wird \cite{kreuzberger2023}.

\subsection{Experimentierzone}

Der Bereich der Experimentierzone umfasst im Kern drei unmittelbar zusammenhängende Aufgaben. 
Dazu gehören die Analyse und Validierung der bereitgestellten Features, 
das iterative Modelltraining mit einem systematischen Parameter-Tuning sowie der Export des bestperformenden Modells in ein Versionsverwaltungssystem, auch Model Registry genannt \cite{kreuzberger2023}.

\subsubsection{Modell-Engineering und Hyperparameter-Optimierung}

\subsubsection{Code-Commit, CI/CD-Trigger und Artefakterstellung}

\subsection{Workflow Automatisierung}

\subsubsection{Automatisches Training, Evaluierung und Registrierung}

\subsubsection{Kontinuierliches Monitoring und Feedback-Schleifen}